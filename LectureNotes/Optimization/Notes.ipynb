{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 Optimization\n",
    "How to find the weight matrices using the loss function?\n",
    "\n",
    "$w^* = argmin_w L(w)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: We're going down an objective landscape basing on the information near us(we can feel the steep but cannot see) \n",
    "### Numeric Gradient\n",
    "![Alt text](image.png)\n",
    "- Initialize W, compute loss\n",
    "- calculate gradient using numeric approximation\n",
    "- Update W\n",
    "- compute a new loss\n",
    "#### Drawbacks\n",
    "- Slow: each update we compute gradient and new loss(large weight matrices and high-dimentional)\n",
    "- Approximate\n",
    "### Analytic Gradient\n",
    "![Alt text](image-1.png)\n",
    "Use backpropagation\n",
    "- exact\n",
    "- fast\n",
    "- error-prone\n",
    "\n",
    "**Gradient check**\n",
    "So in practice, we always use analytic gradient, but check implementation with numerical gradient.      \n",
    "```torch.autograd.gradcheck```\n",
    "\n",
    "## Gradient Descent\n",
    "Idea: Iteratively step in the direction of the negative gradient.\n",
    "```python\n",
    "w = initialize_weights()\n",
    "for i in range(num_steps):\n",
    "    dw = compute_gradient(loss_fn, data, w)\n",
    "    w -= learning_rate * dw\n",
    "```\n",
    "### Hyperparameters\n",
    "- Weight initialization method\n",
    "- Number of steps\n",
    "- Learning rate(how much we trust the gradient to take a how big a step)\n",
    "\n",
    "### Batch Gradient Descent\n",
    "The loss function is the giant sum for all examples in the training dataset.    \n",
    "Simply computing the loss function for each step of iteration is expensive.    \n",
    "![Alt text](image-2.png)\n",
    "\n",
    "### Stochastic Gradient Descent(SGD)\n",
    "Idea: Approximate sum of loss function using a minibatch(32/64/128) of examples in the training set.\n",
    "```python\n",
    "w = initialize_weights()\n",
    "for i in range(num_steps):\n",
    "    minibatch = sampled_data(data, batch_size)\n",
    "    dw = compute_gradient(loss_fn, minibatch, w)\n",
    "    w -= learning_rate * dw\n",
    "```\n",
    "\n",
    "#### Hyperparameters\n",
    "- Weight initialization method\n",
    "- Number of steps\n",
    "- Batch size\n",
    "- Data sampling\n",
    "- Learning rate\n",
    "\n",
    "![Alt text](image-3.png)\n",
    "We can understand SGD basically using Monto Carlo. The N is the number of samples taken to approximate expectation\n",
    "\n",
    "#### Problems with SGD\n",
    "**High conditional number**      \n",
    "Chances are that loss changes quickly in one direction and slowly in another; \n",
    "If we set the learning rate too small -> converging slowly\n",
    "If we set too large -> zig zag to converge\n",
    "![Alt text](image-4.png)\n",
    "The problem is called that the loss function has high **condition number**: ratio of largest to smallest singular value of the Hessian matrix is large.    (!!!!???)    \n",
    "**Local minimum/Saddle point**   \n",
    "![Alt text](image-5.png)\n",
    "**Uncontrolled Stochastic**\n",
    "The gradients may not correlate well with the true direction we want to go.\n",
    "![Alt text](image-6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD + Momentum\n",
    "![Alt text](image-7.png)\n",
    "![Alt text](image-8.png)\n",
    "\n",
    "Momentum can help solving the problems with SGD\n",
    "(???In fact not very clear)\n",
    "![Alt text](image-9.png)\n",
    "\n",
    "Intuitional understanding of SGD + Momentum\n",
    "![Alt text](image-10.png)\n",
    "![Alt text](image-11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad(Adaptive gradient)\n",
    "```python\n",
    "grad_squared = 0\n",
    "for t in range(num_steps):\n",
    "    dw = compute_gradient(w)\n",
    "    grad_squared += dw * dw\n",
    "    w -= learning_rate * dw / (grad_squared.sqrt() + 1e-7)\n",
    "```\n",
    "When in one direction, the gradient is changing fast, the the adagrad would end up dividing by a large value; otherwise small.\n",
    "\n",
    "But the grad_squared may end up too big to make progress before we get to the bottom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp\n",
    "```python\n",
    "grad_squared = 0\n",
    "for t in range(num_steps):\n",
    "    dw = compute_gradient(w)\n",
    "    grad_squared = decay_rate * grad_squared + (1 - decay_rate) * dw * dw\n",
    "    w = -= learning_rate * dw\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam(almost) = RMSProp + Momentum\n",
    "```python\n",
    "moment1 = 0\n",
    "moment2 = 0\n",
    "for t in range(num_steps):\n",
    "    dw = compute_gradient(w)\n",
    "    moment1 = beta1 * moment1 + (1 - beta1) * dw\n",
    "    moment2 = beta2 * moment2 + (1 - beta2) * dw *dw\n",
    "    w -= learning_rate * moment1 / (moment2.sqrt() + 1e-7)\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
