{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Linear Classifiers\n",
    "After moving beyond linear classifier models, the individual components of the complicated NN models would be similar to linear classifiers.\n",
    "\n",
    "Linear classifier is a sort of parametric model/approach.   \n",
    "$$f(x,W)_{(10,)} = W_{(10,3072)}x_{(3072,)} + b_{(10,)}$$ \n",
    "(Note the response variable is multi-dimentional)     \n",
    "Where 10 is the classes we want to label the pic, 3072 is the result of 3 rgb dimensions streched into 1-d(vectorization). \n",
    "\n",
    "## Interpreting for Linear Classifiers\n",
    "### Algebraic view   \n",
    "- We can easily predict the bias part by adding an additional 1-d to the explanatory vectors.   \n",
    "Though this may bring inconvenience to further convoluntional operations, thus often unused in CV.\n",
    "\n",
    "- Predictions are linear.\n",
    "$$f(cx, W) = W(cx) = c*f(x, W)$$\n",
    "![Alt text](image.png)\n",
    "Though scaling down the pixels, we human could still classify the pic; But it reduces the predictive score, which requires us to reflect on the loss function that we use to train the model.\n",
    "\n",
    "### Visual Viewpoint    \n",
    "After reshaping each line of the weight matrix, we can consider each new matrix as an image template for each category.  \n",
    "And this gives us more intuition of what the classifiers are looking for.\n",
    "![Alt text](image-1.png)\n",
    "![Alt text](image-2.png)\n",
    "(But I was wondering the pics below are directly from the weight matrix or the matched data that maximize the corresponding weights?)    \n",
    "And it is interesting that the classifiers focus not only on the object they're searching for but on the **context** cues from the images. \n",
    "### Geometric Viewpoint\n",
    "We compute the inner product between the template and the image vector with unit norm.    \n",
    "The category score reaches highest when they're lined up.    \n",
    "We can consider the category decision boundaries as hyperplanes cutting the space, and the category template \"lines\" go orthogonal to these hyperplanes.\n",
    "![Alt text](image-3.png) \n",
    "### Statistical Viewpoint\n",
    "Amazing! Different people with different backgound would understand the models from different points.      \n",
    "OK This is involved in cross-entropy loss part. \n",
    "\n",
    "## Hard Cases for a Linear Classifier\n",
    "![Alt text](image-4.png)\n",
    "Using geometric viewpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "![Alt text](image-6.png)\n",
    "### Multiclass SVM Loss\n",
    "idea: The score of the correct class should be higher than all the other scores.    \n",
    "data:$(x_i, y_i)$      \n",
    "Let $s = f(x_i, W)$ be scores    \n",
    "The SVM loss has the form:$L_i = \\sum_{j\\ne y_i}max(0, s_j + 1 - s_{y_i})$\n",
    "\n",
    "\n",
    "- Change the score of the correct class a bit won't make much difference to the loss function(robustness?)\n",
    "- A useful debugging trick: what loss function we expect to see, if all of the scores are approximately random\n",
    "- min loss = 0, max = $\\infty$\n",
    "- Let the loss func add all categories or compute the mean not the sum, this just lead to same preferences, the changes are trivial.\n",
    "- But change if we do square sum then the loss func is no more SVM loss\n",
    "- $W$ is not unique if $L = 0$ e.g. $2W$   \n",
    "Then how do we choose between different weight?\n",
    "\n",
    "### Regularization\n",
    "$$L(W) = \\frac1N \\sum_{i=1}^N L_i(f(x_i,W), y_i) + \\lambda R(W)$$\n",
    "$\\lambda = regularization strength$(hyperparameter)      \n",
    "We use \\lambda to set the trade-off between how well we want the model to fit the training data and how well we want the model to achive the regularization loss.\n",
    "\n",
    "Examples:\n",
    "![Alt text](image-7.png)\n",
    "Purpose:     \n",
    "- Express our preferences among models beyond \"minimize training loss\"(give some hints on what we want; we can inject some prior knowledge beyond data here)\n",
    "![Alt text](image-8.png)\n",
    "L2 is more sensitive to variation\n",
    "- Avoid overfitting\n",
    "![Alt text](image-10.png)\n",
    "- Improving optimization by adding curvature(?)\n",
    "\n",
    "### Cross-Entropy Loss\n",
    "We want to interpret raw classifier scores as probabilities.\n",
    "$$s = f(x_i;W)$$\n",
    "$$P(Y=k|X=x_i) = \\frac{e^{s_k}}{\\sum_j e^{s_j}}softmax$$\n",
    "A differentiable approximation to the max function, compensating for bad math properties for hardmax.\n",
    "$$ L_i = P(Y=y_i|X=x_i) = -log\\frac{e^{s_i}}{\\sum_j e^{s_j}}$$\n",
    "(For detailed information about cross-entropy loss, see sufeDL2023fa lec2.)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
